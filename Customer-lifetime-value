#load dataset 

df <- read.csv("CLV.csv")
library(tidyverse)
###################
# Business understanding
##################
#we want to model the customer lifetime value, that means the value we 
#predict to get out of a customer to see if it worth to spend ads on him 

#################
# data exploration
################

#analysis with IBM Cognos analytics tool shows that most important features are
#number of policies, vehicle class, monthly premium auto coverage and total claim amount 

#so at first we'll reduce some of the columns 
str(df, give.attr=FALSE)
df <- subset(df, select=-Customer)
df <- subset(df, select=-Education)
df <- subset(df, select=-Gender)
df <- subset(df, select=-State)
df <- subset(df, select=-Effective.To.Date)
df <- subset(df, select=-Sales.Channel)



library(tidyverse)
summary(df)
head(df)

#Missing values?
sapply(df, function(x) sum(is.na(x)))

#target Variable 
df <- rename(df, target = Customer.Lifetime.Value)

################
#data understanding
#################
#Gender
ggplot(df, aes(Gender)) +
  geom_bar()
#similiar total number, maybe no real correlation with CLV, check aftertransfromation

#state 
ggplot(df, aes(State)) +
  geom_bar() +
  coord_flip()

#Income
summary(df$Income)
ggplot(df, aes(Income, fill="red")) +
  geom_histogram(binwidth = 5000)

#response
unique(df$Response)
prop.table(table(df$Response))

#Education 
prop.table(table(df$Education))

#target
summary(df$target)
ggplot(df, aes(target, fill="red")) +
  geom_histogram(binwidth = 500)

#marital status
prop.table(table(df$Marital.Status))

#employment status 
prop.table(table(df$EmploymentStatus))


#################
#data transformation
##################
#urban
prop.table(table(df$Location.Code))
df$Location.Code[df$Location.Code=="Suburban"] <- 1
df$Location.Code[df$Location.Code=="Rural"] <- 1
df$Location.Code[df$Location.Code=="Urban"] <- 0
df$Location.Code <- as.numeric(df$Location.Code)

#policy type 
df$Policy.Type[df$Policy.Type=="Special Auto "] <- 1
df$Policy.Type[df$Policy.Type=="Personal Auto"] <- 1
df$Policy.Type[df$Policy.Type=="Corporate Auto"] <- 0
df$Policy.Type <- as.numeric(df$Policy.Type)


#gender
unique(df$Gender)
df$Gender[df$Gender=="M"] <- 1
df$Gender[df$Gender=="F"] <- 0
df$Gender <- as.numeric(df$Gender)
head(df$Gender) #proof

cor(df$Gender, df$target, method="pearson") 
#really weak correlation, we can remove it
df <- subset(df, select=-Gender)


#response
df$Response[df$Response=="Yes"] <- 1
df$Response[df$Response=="No"] <- 0
df$Response <- as.numeric(df$Response)

#employment into employed and unemployed 
df$EmploymentStatus <- gsub("Medical Leave", "Unemployed", df$EmploymentStatus)
df$EmploymentStatus <- gsub("Disabled", "Unemployed", df$EmploymentStatus)
df$EmploymentStatus <- gsub("Retired", "Unemployed", df$EmploymentStatus)
#and turn into numbers 
df$EmploymentStatus[df$EmploymentStatus=="Employed"] <- 1
df$EmploymentStatus[df$EmploymentStatus=="Unemployed"] <- 0
df$EmploymentStatus <- as.numeric(df$EmploymentStatus)


#marital status 
df$Marital.Status <- gsub("Divorced", "Single", df$Marital.Status)
df$Marital.Status[df$Marital.Status=="Married"] <- 1
df$Marital.Status[df$Marital.Status=="Single"] <- 0
df$Marital.Status <- as.numeric(df$Marital.Status)

#catgorical data into k-1 dummies 
unique(df$State)#5
unique(df$Coverage) #3
unique(df$Education) #5
unique(df$Vehicle.Size)


#dummies
#vehicle size 
df$Vehicle.Size.Small <- ifelse(df$Vehicle.Size == "Small", 1, 0)
df$Vehicle.Size.Large <- ifelse(df$Vehicle.Size == "Large", 1, 0)
df <- subset(df, select=-Vehicle.Size)

#Coverage
df$Coverage.Basic <- ifelse(df$Coverage == "Basic", 1, 0)
df$Coverage.Premium <- ifelse(df$Coverage == "Premium", 1, 0)
df <- subset(df, select=-Coverage)

#Renew.Offer.Type  
df$Renew.Offer1 <- ifelse(df$Renew.Offer.Type  == "Offer1", 1, 0)
df$Renew.Offer2  <- ifelse(df$Renew.Offer.Type  == "Offer2", 1, 0)
df$Renew.Offer3  <- ifelse(df$Renew.Offer.Type  == "Offer3", 1, 0)
df <- subset(df, select=-Renew.Offer.Type)
str(df)


#Vehicle.Class     
unique(df$Vehicle.Class)
df$Vehicle.2door <- ifelse(df$Vehicle.Class  == "Two-Door Car", 1, 0)
df$Vehicle.4door  <- ifelse(df$Vehicle.Class  == "Four-Door Car", 1, 0)
df$Vehicle.SUV  <- ifelse(df$Vehicle.Class  == "SUV", 1, 0)
df$Vehicle.Lux_SUV <- ifelse(df$Vehicle.Class  == "Luxury SUV", 1, 0)
df$Vehicle.Sport_Car  <- ifelse(df$Vehicle.Class  == "Sports Car", 1, 0)
df <- subset(df, select=-Vehicle.Class)

#policy
df <- subset(df, select=-Policy)

###########################
#correlations between independent X Variables (must be under 0.5)
library(corrplot)
corrplot(cor(df), method = "number")
cor(df, method="pearson")

#or
df %>%
  select_if(is.numeric) %>%
  cor() %>%
  corrplot(method="number")


#income and employment status 
df <- subset(df, select=-EmploymentStatus)




####################
#Model Validation with training and testing set
###################
library(caret)

set.seed(123)
intrain <- createDataPartition(df$target, p=0.7, list=FALSE)
training <- df[intrain,]
testing <- df[-intrain,]

#confirm the splitting is correct
dim(training); dim(testing) 


######################
#Model Building: Multiple logistic regression
######################
names(df)

#backward elimination
full.model <- lm(target ~., data=na.omit(training))
step(full.model, direction = "backward", trace=FALSE)

#building model with only statistical significant variables
lm.clv <-  lm(target ~ Income + Monthly.Premium.Auto + Number.of.Open.Complaints + 
     Number.of.Policies + Policy.Type + Total.Claim.Amount + Renew.Offer1 + 
     Renew.Offer2 + Renew.Offer3 + Vehicle.2door + Vehicle.4door + 
     Vehicle.SUV + Vehicle.Sport_Car, training)
summary(lm.clv)


# top 3 features with lowest p value:
#linear regression w/ Monthly.Premium.Auto, Number.of.Open.Complaints, renew.offer
lm.clv.2 <- lm(target ~ Monthly.Premium.Auto + Number.of.Open.Complaints +
               Renew.Offer1 + Renew.Offer2 +Renew.Offer3, training )
summary(lm.clv.2)


#lets check the deviance table 
anova(lm.clv, test="Chisq")

####################
#prediction
###################
#prediction on training



#prediction on testing 
#lets make new columns for the predictions and for the difference between real CLV and prediction
testing.prediction <- testing %>%
  mutate(prediction = predict(lm.clv)[1:2739]) %>% #bc prediction has more rows than testing
  mutate(false.prediction = target - prediction) %>%
  select(target, prediction, false.prediction )
view(testing.prediction)#proof
summary(testing.prediction$false.prediction)

#Root mean squared Error
RMSE <- sqrt(mean(testing.prediction$false.prediction^2))
RMSE

#Error rate 
error.rate <- abs((testing.prediction$false.prediction)/(testing.prediction$target)*100)
mean(error.rate)


###################
#conclusion:
###################

#the data can not determine very well customer lifetime value only 16.2% of the variation of 
#the independent variable (CLV) can be explained through them. The most important features 
#include Monthly.Premium.Auto, Number.of.Open.Complaints and Renew.Offer. So if you want 
#to improve the Customer Lifetime Value you should pay attention to them.













